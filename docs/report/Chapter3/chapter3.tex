\chapter{Evaluation}

The final chapter, critically evaluates the work done over the course of the project.

\subsection{What Went Well}
Every software has its strengths and weaknesses, which also applies to the Quiz Tool.
The application has been developed by putting the importance on implementing
features in the simplest way possible. The ambition of making Qwizdom redundant
in the future, and making Quiz Tool the main tool Aberystwyth University uses
for running interactive lectures, forced me to think about the future maintainability
of the application at all stages of the design and development.

All of the proposed tasks identified in the outline project specification have been
completed, and the tool has been successfully implemented to address all of the
top level functional requirements identified in the early stages of the development.
The single person adjusted SCRUM methodology proved to be successful in helping
to structure the work over the weeks. Weekly sprints enabled me to stay on track
and deliver the working software on time before the deadline. Being able to discuss
my choices with the client was also enormously helpful, and complemented the
SCRUM methodology chosen.

Choosing to implement the tool as a web application, using entirely JavaScript
based technologies was a good decision. If the mobile application approach has
been chosen as initially proposed, it would be unlikely for the application to be
as well developed as using the MEAN stack. The initial high learning curve of
understanding Angular, and deciding how to structure the back end of the application
written in Express, paid off later on as the development was fairly straightforward.
If on the other hand, extra frameworks would have had to be learnt before the development
could started, the amount of time for the technical part of the project would most
likely not be enough to finish it on time. Furthermore, the decision to tackle the most technically
difficult issues first, by prototyping during the early stages of the development
was a smart move. If easy tasks were tackled first, there could have been the
danger of having multiple, unclear requirements towards the end of the process, which
could lead to going over the deadline.

Putting the DevOps infrastructure in place during the very first sprint, and structuring
the application using docker-compose, and then automatically deploying it to the
production environment provided by AWS was also a good choice. The build agent
could give reassurance the software was not degrading in quality by running tests
whenever a pull request was made on GitHub. Having Docker containers running in a
similar fashion during all development, testing and production, made the development
much more enjoyable and productive.

A decent amount of time have been put into the development of the tool. The time
commitment has been tracked on daily basis using the spreadsheet included in the
\autoref{chap:timesheet} of this report, which
made sprint planning easier, as the velocity of the development could be monitored.
GitHub issues could be then estimated with a greater accuracy, although some
sprints did go over a single week. Other than the two sprints which lasted two weeks,
the amount of work allocated into each sprint was optimal.

Quiz Tool's main selling point, compared to Qwizdom is the ability to help lecturers
in the creation of quizzes. Because the text of each slide is extracted, and the tool
knows how to render eligible slides as quizzes, the lecturer only has to mark
slides as multi choice quizzes and the rest is handled automatically. It is very quick
to upload a set of slides, and as long as slides follow the expected format of having
A, B, C letters, or bullet points, the tool can make life easier for the end user.

Finally, the testing strategy was mainly correct. Adding tests lead to defects being
found and fixed, which improved the overall quality of the tool, and the confidence
of its suitability to run in the production. A successful lecture has been run using
the tool, and the application successfully handled a real world scenario of handling
over fifty users. The user feedback was also mainly positive.

\subsection{What Could Have Been Improved}
Looking back, certain parts of the Quiz Tool could have been implemented differently,
or perhaps should be improved if the tool outlived the final project submission, and was
to be actually used by the university.

The main limitation of the tool in its current state is that the file upload
is limited to 15 MB. This means that no PDF presentation over 15MB can be uploaded
and used successfully with the tool. The problem comes down to the choice of
focusing on making the design as simple as possible, while still delivering a
product to the desired standard. Files are stored in the MongoDB database directly
in the BSON format. The maximum size of the BSON property stored in an object in
MongoDB is 16MB, but the upload limit has been set to 15MB, to be extra safe the tool could
handle the uploaded file. I wanted to avoid introducing GridFS, capable of
splitting binary files into small chunks and storing them in the database, due
to the significant increase in the complexity of the design and implementation.
The third approach which I should have probably chosen instead, was to store files
on disk, and keep references to these files in the database. If I could go back
and re-implement this part of the system, I would have gone with the third option.
This could be still re-implemented if enough time was available, and the use case
of uploading files over 15MB was necessary.

The final discussion with the client highlighted certain improvements that could
be made to the tool to make it more usable. For example when the lecturer edits
a lecture to mark certain lectures as quizzes, he has to scroll all the way up to
save or discard his changes. An extra set of buttons on the bottom of the screen
could address this issue. Or perhaps having the buttons fixed on the screen
in one position could be the answer. Another issue, is the way slides are broadcasted
to the audience, or actually how the answers to quizzes are stored in memory.
As long as the lecturer does not go back in the presentation, the tool works
as expected. The problem could appear when answers were collected, and then the
lecturer decides to navigate to the first slide of the presentation having shown half
of it before. The design of the tool, resets previously submitted answers if
the lecturer decides to go back past the previously answered quiz. This could be addressed by
keeping previous questions, and making the lecturer to explicitly reset the
quiz if he wants to.

Even though the development method chosen worked well for the project, two
sprints were overestimated, as too many issues were allocated to a single week.
This was not too problematic, as the sprints were simply extended to last an extra
week.

Finally, the overall testing strategy was correct. However the front end Angular
tests should have probably been more extensive. The UI could have been tested
using the Angular tests to complement the selenium tests, rather than relying
on the selenium tests to test the front end. Both Angular unit tests, and integration
tests should have been added in the early sprints of the development, as opposed
the very last one. This would have made the continuous integration able to
spot defects earlier, and give more confidence into the correctness of the implementation
early on.

% - File upload over 15 MB
% - File upload and splitting to slides takes a decent amount of time before
% the user can use the tool. User interface could have been improved to let the
% user know everything is fine, or submit the task to split lecture slides and then
% let the lecturer
% - LDAP vs Google Single Sign On unfortunate lack of resources
%  - nothing really stops students, or malicios users from loggin in and attempting to crash the software
% - Two sprints were overestimated and resulted in two weeks long sprints as opposed to
% one week
% - Angular unit tests could be extended
% - Front end and integration tests should have been added earlier. They were added in the
% last iteration which was a risky strategy
% - potentially more comments

% Examiners expect to find in your dissertation a section addressing such questions as:
%
% \begin{itemize}
%    \item Were the requirements correctly identified?
%    \item Were the design decisions correct?
%    \item Could a more suitable set of tools have been chosen?
%    \item How well did the software meet the needs of those who were expecting to use it?
%    \item How well were any other project aims achieved?
%    \item If you were starting again, what would you do differently?
% \end{itemize}
%
% Other questions can be addressed as appropriate for a project.
%
% Such material is regarded as an important part of the dissertation; it should demonstrate
% that you are capable not only of carrying out a piece of work but also of thinking critically
%  about how you did it and how you might have done it better. This is seen as an important part of an honours degree.
%
% There will be good things and room for improvement with any project. As you write this section,
%  identify and discuss the parts of the work that went well and also consider ways in which the work could be improved.
%
% In the latter stages of the module, we will discuss the evaluation. That will probably
% be around week 9, although that differs each year.
%
% Could have been improved:
% - file uplaod
